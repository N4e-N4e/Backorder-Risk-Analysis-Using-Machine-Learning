{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import  mutual_info_classif,SelectPercentile\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the balanced sample and the best pipeline and the anomaly detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smart sample\n",
    "dataset = pd.read_csv('newdf.csv').sample(frac = 1).reset_index(drop=True)\n",
    "dataset.head()\n",
    "\n",
    "pipe = pickle.load(open(\"best_pipeline.pkl\", \"rb\"))\n",
    "envelope= pickle.load(open(\"envelope_AD_model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Retrain pipeline using the full balanced sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "X = dataset.drop(columns='went_on_backorder')\n",
    "y = dataset['went_on_backorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling:\n",
    "numerical_cols = ['national_inv', 'lead_time', 'in_transit_qty', 'forecast_6_month', 'sales_6_month', \n",
    "                  'min_bank', 'pieces_past_due', 'perf_6_month_avg', 'local_bo_qty']\n",
    "categorical_cols = ['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_scale_train = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "X_= np.hstack((num_scale_train, X[categorical_cols].values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Outliers = 459\n"
     ]
    }
   ],
   "source": [
    "#Anomaly Detection:\n",
    "out_envelope = envelope.predict(X_) == -1\n",
    "\n",
    "print(f\"No. of Outliers = {np.sum(out_envelope)}\")\n",
    "X_clean = X_[~out_envelope]\n",
    "y_clean = y[~out_envelope]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('select',\n",
       "                 SelectPercentile(percentile=50,\n",
       "                                  score_func=<function mutual_info_classif at 0x7f0efeef2048>)),\n",
       "                ('gb',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, max_depth=5,\n",
       "                                            min_samples_split=10,\n",
       "                                            random_state=42, subsample=0.6))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_clean, y_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle and save the trained model and the anomaly detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline - Model and Parameters\n",
    "pickle.dump(pipe, open(\"Final_trained_pipeline.pkl\", \"wb\"))\n",
    "\n",
    "#Anomaly Detection model\n",
    "pickle.dump(envelope, open(\"Final_trained_envelope_AD_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Load the test data and evaluating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3510135</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3309787</td>\n",
       "      <td>102.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3451379</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3433288</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3453698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku  national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0  3510135          19.0        8.0             0.0               0.0   \n",
       "1  3309787         102.0       52.0             0.0               0.0   \n",
       "2  3451379         101.0        8.0            76.0              60.0   \n",
       "3  3433288          16.0        2.0             0.0               0.0   \n",
       "4  3453698           4.0        8.0             4.0               0.0   \n",
       "\n",
       "   forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "0              22.0              44.0            1.0           18.0   \n",
       "1               0.0               0.0            3.0            8.0   \n",
       "2              60.0             120.0           27.0           58.0   \n",
       "3               0.0               0.0            0.0            0.0   \n",
       "4               0.0               3.0            3.0            6.0   \n",
       "\n",
       "   sales_6_month  ...  pieces_past_due  perf_6_month_avg perf_12_month_avg  \\\n",
       "0           45.0  ...              0.0              0.91              0.95   \n",
       "1           19.0  ...              0.0              1.00              0.99   \n",
       "2           99.0  ...              0.0              0.90              0.86   \n",
       "3            0.0  ...              0.0              0.70              0.46   \n",
       "4            9.0  ...              0.0              0.99              0.99   \n",
       "\n",
       "   local_bo_qty  deck_risk  oe_constraint  ppap_risk stop_auto_buy rev_stop  \\\n",
       "0           0.0         No             No         No           Yes       No   \n",
       "1           0.0        Yes             No         No           Yes       No   \n",
       "2           0.0         No             No         No           Yes       No   \n",
       "3           0.0         No             No         No           Yes       No   \n",
       "4           0.0         No             No         No           Yes       No   \n",
       "\n",
       "  went_on_backorder  \n",
       "0                No  \n",
       "1                No  \n",
       "2                No  \n",
       "3                No  \n",
       "4                No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the given test set \n",
    "# ----------------------------------\n",
    "\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv'\n",
    "\n",
    "data = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242076, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensions\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the given test set \n",
    "# ----------------------------------\n",
    "#Droping columns to prevent multicollinearity and that are no needed:\n",
    "remove = ['sku','forecast_3_month', 'forecast_9_month', 'sales_1_month', 'sales_3_month', 'sales_9_month', 'perf_12_month_avg']\n",
    "data = data.drop(remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_6_month',\n",
      "       'sales_6_month', 'min_bank', 'potential_issue', 'pieces_past_due',\n",
      "       'perf_6_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
      "       'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "national_inv             1\n",
       "in_transit_qty           1\n",
       "forecast_6_month         1\n",
       "sales_6_month            1\n",
       "min_bank                 1\n",
       "potential_issue          1\n",
       "pieces_past_due          1\n",
       "perf_6_month_avg         1\n",
       "local_bo_qty             1\n",
       "deck_risk                1\n",
       "oe_constraint            1\n",
       "ppap_risk                1\n",
       "stop_auto_buy            1\n",
       "rev_stop                 1\n",
       "went_on_backorder        1\n",
       "lead_time            14725\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding null null values in columns:\n",
    "data.isnull().sum().sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6% of rows contain null value in feature 'lead_time', and as such, will be removed.\n",
    "data = data.dropna(subset=['lead_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "national_inv         0\n",
       "lead_time            0\n",
       "in_transit_qty       0\n",
       "forecast_6_month     0\n",
       "sales_6_month        0\n",
       "min_bank             0\n",
       "potential_issue      0\n",
       "pieces_past_due      0\n",
       "perf_6_month_avg     0\n",
       "local_bo_qty         0\n",
       "deck_risk            0\n",
       "oe_constraint        0\n",
       "ppap_risk            0\n",
       "stop_auto_buy        0\n",
       "rev_stop             0\n",
       "went_on_backorder    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking......\n",
    "data.isnull().sum().sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "potential_issue      2\n",
       "deck_risk            2\n",
       "oe_constraint        2\n",
       "ppap_risk            2\n",
       "stop_auto_buy        2\n",
       "rev_stop             2\n",
       "went_on_backorder    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for unique value count, to ensure no additional class for the object data types. \n",
    "data.nunique().sort_values().head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n"
     ]
    }
   ],
   "source": [
    "# All the column names of these yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: data[i].dtype!=np.float64, data.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Fill missing values if any\n",
    "for column_name in yes_no_columns:\n",
    "    mode = data[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    data[column_name].fillna(mode, inplace=True)\n",
    "\n",
    "# Convert 'Yes'/'No' to 1/0\n",
    "for column_name in yes_no_columns:\n",
    "    data[column_name] = data[column_name].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>national_inv</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>494.771917</td>\n",
       "      <td>30146.679689</td>\n",
       "      <td>-25414.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>79.00</td>\n",
       "      <td>12145792.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_time</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>7.923018</td>\n",
       "      <td>7.041410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_transit_qty</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>36.922037</td>\n",
       "      <td>741.337000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>186624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_6_month</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>369.731842</td>\n",
       "      <td>10401.026280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2157024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_6_month</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>349.273339</td>\n",
       "      <td>9227.229225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2103389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_bank</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>53.315732</td>\n",
       "      <td>1152.838833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>196869.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potential_issue</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.018988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pieces_past_due</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>1.941249</td>\n",
       "      <td>184.373362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>-1.141629</td>\n",
       "      <td>13.712759</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_bo_qty</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.886326</td>\n",
       "      <td>47.004905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deck_risk</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.169940</td>\n",
       "      <td>0.375581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oe_constraint</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppap_risk</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.116723</td>\n",
       "      <td>0.321090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.972285</td>\n",
       "      <td>0.164155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_stop</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went_on_backorder</th>\n",
       "      <td>227351.0</td>\n",
       "      <td>0.011454</td>\n",
       "      <td>0.106407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count        mean           std      min  25%    50%  \\\n",
       "national_inv       227351.0  494.771917  30146.679689 -25414.0  4.0  15.00   \n",
       "lead_time          227351.0    7.923018      7.041410      0.0  4.0   8.00   \n",
       "in_transit_qty     227351.0   36.922037    741.337000      0.0  0.0   0.00   \n",
       "forecast_6_month   227351.0  369.731842  10401.026280      0.0  0.0   0.00   \n",
       "sales_6_month      227351.0  349.273339   9227.229225      0.0  0.0   3.00   \n",
       "min_bank           227351.0   53.315732   1152.838833      0.0  0.0   0.00   \n",
       "potential_issue    227351.0    0.000361      0.018988      0.0  0.0   0.00   \n",
       "pieces_past_due    227351.0    1.941249    184.373362      0.0  0.0   0.00   \n",
       "perf_6_month_avg   227351.0   -1.141629     13.712759    -99.0  0.7   0.84   \n",
       "local_bo_qty       227351.0    0.886326     47.004905      0.0  0.0   0.00   \n",
       "deck_risk          227351.0    0.169940      0.375581      0.0  0.0   0.00   \n",
       "oe_constraint      227351.0    0.000207      0.014377      0.0  0.0   0.00   \n",
       "ppap_risk          227351.0    0.116723      0.321090      0.0  0.0   0.00   \n",
       "stop_auto_buy      227351.0    0.972285      0.164155      0.0  1.0   1.00   \n",
       "rev_stop           227351.0    0.000264      0.016243      0.0  0.0   0.00   \n",
       "went_on_backorder  227351.0    0.011454      0.106407      0.0  0.0   0.00   \n",
       "\n",
       "                     75%         max  \n",
       "national_inv       79.00  12145792.0  \n",
       "lead_time           9.00        52.0  \n",
       "in_transit_qty      0.00    186624.0  \n",
       "forecast_6_month   16.00   2157024.0  \n",
       "sales_6_month      33.00   2103389.0  \n",
       "min_bank            4.00    196869.0  \n",
       "potential_issue     0.00         1.0  \n",
       "pieces_past_due     0.00     79964.0  \n",
       "perf_6_month_avg    0.97         1.0  \n",
       "local_bo_qty        0.00      6232.0  \n",
       "deck_risk           0.00         1.0  \n",
       "oe_constraint       0.00         1.0  \n",
       "ppap_risk           0.00         1.0  \n",
       "stop_auto_buy       1.00         1.0  \n",
       "rev_stop            0.00         1.0  \n",
       "went_on_backorder   0.00         1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4377, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data['perf_6_month_avg'] == -99)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few rows have a performance score of -99, which falls outside the expected range of 0 to 1. \n",
    "#It could be that it's likely either a placeholder or invalid data.\n",
    "#It will be removed it to avoid any sort of distortion.\n",
    "\n",
    "data = data[data['perf_6_month_avg'] != -99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222974, 16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensions\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the data in X_test and y_test\n",
    "X_test = data.drop(columns='went_on_backorder')\n",
    "y_test = data['went_on_backorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222974, 15)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling:\n",
    "num_scale_test = scaler.transform(X_test[numerical_cols])\n",
    "X_test_scaled= np.hstack((num_scale_test, X_test[categorical_cols].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08432486,  0.17465271, -0.05550266, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.03940284,  8.13891765, -0.05550266, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.03994406,  0.17465271,  0.0820374 , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.09136083,  0.17465271, -0.05550266, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.08486608,  0.8986768 , -0.05550266, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.09190206, -0.91138342, -0.05550266, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking......\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. \n",
    "\n",
    "Report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Outliers:\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code  \n",
    "# ----------------------------------\n",
    "y_pred_w = pipe.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[193710  26700]\n",
      " [   527   2037]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93    220410\n",
      "           1       0.07      0.79      0.13      2564\n",
      "\n",
      "    accuracy                           0.88    222974\n",
      "   macro avg       0.53      0.84      0.53    222974\n",
      "weighted avg       0.99      0.88      0.93    222974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, y_pred_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8778915927417547\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Outliers (Comparison purposes)\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Outliers = 10850\n"
     ]
    }
   ],
   "source": [
    "#Comparison purposes\n",
    "outliers = envelope.predict(X_test_scaled) == -1\n",
    "X_test_clean = X_test_scaled[~outliers]\n",
    "y_test_clean = y_test[~outliers]\n",
    "print(f\"No. of Outliers = {np.sum(outliers)}\")\n",
    "\n",
    "y_pred_woo = pipe.predict(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[183726  25937]\n",
      " [   498   1963]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "print(confusion_matrix(y_test_clean, y_pred_woo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93    209663\n",
      "           1       0.07      0.80      0.13      2461\n",
      "\n",
      "    accuracy                           0.88    212124\n",
      "   macro avg       0.53      0.84      0.53    212124\n",
      "weighted avg       0.99      0.88      0.92    212124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test_clean, y_pred_woo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8753794950123512\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test_clean, y_pred_woo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Processing:\n",
    "----------------------------------\n",
    "Missing/Invalid Data: Handled in Part 1. Rows containing missing values or invalid entries (e.g.,-99) in any feature were removed.\n",
    "\n",
    "Data Transformation: Categorical variables with classes \"Yes\" and \"No\" were transformed into a binary format (1/0). \n",
    "\n",
    "Smart Sample: A random sample size of 10000 instances was drawn from the balanced dataset created in Part 1.\n",
    "\n",
    "Feature Scaling:  Only non-binary numeric features were scaled using StandardScaler.\n",
    "\n",
    "Anomaly Detection: EllipticEnvelope was applied to remove noise and extreme values, helping the model improve its generalization capability by training on clean data.\n",
    "\n",
    "Feature Selection:  SelectPercentile was used to retain the top 50% most relevant features. \n",
    "\n",
    "Model Tuning: Hyperparameter tuning and cross-validation were performed in Part 2 to enhance performance and reduce overfitting.\n",
    "\n",
    "Model Training: A GradientBoostingClassifier was used, as it formed part of the pipeline that provided the best performance in Part 2. \n",
    "The model was trained on the cleaned, balanced training dataset to learn and identify patterns associated with the target variable (went_on_backorder).\n",
    "\n",
    "Performance Evaluation: The final evaluation was conducted on entirely unseen data. Preprocessing was done on the test set, but outliers were retained to test the model's robustness on real-world-like data.\n",
    "\n",
    "Evaluation Metrics: A Confusion Matrix and Classification Report were used to evaluate performance on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance:\n",
    "-----------------------\n",
    "The model, as mentioned before, was evaluated on test data with outliers included, which reflects the real-world data that is often noisy and imperfect. \n",
    "\n",
    "Key metrics:\n",
    "------\n",
    "Accuracy: 87.8%\n",
    "\n",
    "Precision (Class 1 - Backorder): 7%\n",
    "\n",
    "Recall (Class 1 - Backorder): 79%\n",
    "\n",
    "F1-Score (Class 1 - Backorder): 13%\n",
    "\n",
    "\n",
    "Analysis:\n",
    "-----------\n",
    "The model accuracy is inflated and, as such, unreliable. This is due to class imbalance, where the model achieves high accuracy by correctly predicting the majority class (class 0), which accounts for almost 98% of the data.\n",
    "\n",
    "The precision score for predicting class 1 is very low (7%), indicating that many items expected to go on backorder did not (High False Positives).\n",
    "\n",
    "The recall score for class 1 was high (~80%), which suggests that the model correctly identified most of the actual backorder cases.\n",
    "\n",
    "The F1-score for class 1 was low (13%) due to the significant imbalance between precision and recall. \n",
    "\n",
    "Model performance remained nearly the same before and after the removal of test set outliers, suggesting that the model is relatively robust to noise and extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: After seeing the performance in Part 3, how would you change your choices in Part 1?\n",
    "\n",
    "A: In Part 1, the method I used to handle unbalanced data was \"Undersampling\". This removed a large portion of the majority class data,  likely discarding valuable information and contributing to the high false positive value. \n",
    "So, instead of doing a 50/50 split, keeping a slightly imbalanced ratio of 60/40 or 70/30 would have been ideal. This way, more useful information might have been preserved, potentially improving precision and recall.  \n",
    "Alternatively, I could have explored other techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Is your model robust to outliers, or is their number just too small to make a difference?\n",
    "\n",
    "A: When checking for outliers in the test set, I found approximately 10000 instances - about 4.8% of the total data. While this is a small portion, in my opinion, it is not insignificant.\n",
    "Surprisingly, the evaluation scores remained nearly the same with or without outliers, suggesting that the model used is fairly robust to outliers, or at the very least, the detected outliers didn't meaningfully affect the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How do you decide what \"best\" performance is? Accuracy, F1 score, recall, or precision? Keep in mind that this is an imbalanced problem; which is more important: false negatives or false positives?\n",
    "\n",
    "A: From a business point of view, I would say that recall is the best metric we can use to evaluate the model's performance. \n",
    "High recall indicates that the model is good at identifying actual positive cases. \n",
    "In this situation, false negatives can lead to serious consequences, such as dissatisfied customers or loss of sales. On the other hand, false positives more or less result in over-preparation, which is not that harmful to the supplier. Therefore, it might be ideal to maximize recall at the cost of some precision to avoid missing critical backorders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to its high recall performance, the developed model reliably detects products/parts likely to go on backorder, enabling early intervention to prevent loss of sales or customer dissatisfaction. \n",
    "While the model occasionally flags parts/products unnecessarily due to its low precision, this trade-off is reasonable because missing an actual shortage is far more costly than investigating a false alarm. \n",
    "Based on the test results, I am confident in the model's ability to generalize well to new, unseen data. However, it is not meant to replace human oversight. The best approach is to integrate it into the inventory management process as an alert system, where flagged parts/products are prioritized for review by the supply chain team. Combining automation and expert human judgment can help reduce risk and improve resource planning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
